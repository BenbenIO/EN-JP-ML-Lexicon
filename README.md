# Machine Learning and Deep Learning: EN-JP Lexicon 

This is a English-Japanese lexicon for Machine Learning and Deep Learning terminology. Based on the translation work for the [ML](https://github.com/afshinea/stanford-cs-229-machine-learning) and [DL cheatsheets](https://github.com/afshinea/stanford-cs-230-deep-learning) created by @afshinea for Stanford's CS 229 Machine Learning and CS 230 Deep Learning.

# Deep Learning

## Deep Learning basics (from: DL tips and tricks)

| English            | 日本語                 |
|:-------------------|:-----------------------|
| Adaptive learning rates  | 適応学習率 |
| Analytical gradient | 解析的勾配 |
| Architecture | アーキテクチャ |
| Backpropagation       | 誤差逆伝播法                   |
| Batch normalization   | バッチ正規化                   |
| Binary classification | 二項分類 |
| Calculation | 計算 |
| Chain rule | 連鎖律 |
| Coefficients |  係数 |
| Color shift | カラーシフト |
| Contrast change | コントラスト（鮮やかさ）の修正 | 
| Convolution layer | 畳み込み層 | 
| Cross-entropy loss    | 交差エントロピー誤差                   |
| Dampens oscillations | 振動を抑制する | 
| Data augmentation     | データ拡張                   |
| Data processing       | データ処理                   |
| Deep learning         | 深層学習                   |
| Derivative | 微分 |
| Dropout | Dropout (ドロップアウト) |
| Early stopping | Early stopping (学習の早々な終了) |
| Epoch                 | エポック                   |
| Error | 損失 | 
| Evaluation | 評価 |
| Finding optimal weights | 最適な重みの探索 |
| Flip | 反転 |
| Forward propagation | 順伝播 |
| Fully connected layer | 全結合層 |
| Gradient checking     | 勾配チェック     |
| Gradient descent      |  勾配降下法                   |
| Gradient of the loss | 損失の勾配 |
| Hyperparameter | ハイパーパラメータ | 
| Improvement to SGD | SGDの改良 |
| Information loss | 情報損失 | 
| Learning algorithm | 学習アルゴリズム |
| Learning rate         | 学習率 |
| Loss function | 損失関数 | 
| Mini-batch            | ミニバッチ                   |
| Momentum | Momentum（運動量）|
| Neural network training | ニューラルネットワークの学習                   |
| Noise addition | ノイズの付加 | 
| Non-linear layer | 非線形層 |
| Numerical gradient | 数値的勾配 |
| Optimizing convergence | 収束の最適化 |
| Output | 出力 | 
| Overfitting | 過学習 |
| Parameter tuning      | パラメータチューニング | 
| Parameter tuning | パラメータチューニング |
| Parametrize | パラメータ化する |
| Pre-trained weights | 学習済みの重み |
| Prevent overfitting | 過学習を避けるために |
| Random crop | ランダムな切り抜き |
| Regularization | 正規化 |
| Root Mean Square propagation |  二乗平均平方根のプロパゲーション | 
| Rotation | 回転 |
| Transfer learning     | 転移学習 |
| Type | 種類 |
| Updating weights      | 重み更新     |
| Validation loss | バリデーションの損失 |
| Weight regularization | 重みの正規化 |
| Weights initialization | 重みの初期化 |
| Xavier initialization | Xavier初期化 |










## Convolutional Neural Nets

| English            | 日本語                 |
|:-------------------|:-----------------------|
| something          | なになに                   |
| something          | なになに                   |


## Recurrent Neural Nets

| English            | 日本語                 |
|:-------------------|:-----------------------|
| something          | なになに                   |
| something          | なになに                   |



# Machine Learning

## Supervised Learning

| English            | 日本語                 |
|:-------------------|:-----------------------|
| something          | なになに                   |
| something          | なになに                   |


## Unsupervised Learning

| English            | 日本語                 |
|:-------------------|:-----------------------|
| something          | なになに                   |
| something          | なになに                   |


## Probabilities and Statistics

| English            | 日本語                 |
|:-------------------|:-----------------------|
| something          | なになに                   |
| something          | なになに                   |

## Algebra and Calculus

| English            | 日本語                 |
|:-------------------|:-----------------------|
| something          | なになに                   |
| something          | なになに                   |



